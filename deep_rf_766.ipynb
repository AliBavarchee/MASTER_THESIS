{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5231775",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88863104",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f651a89c",
   "metadata": {
    "cell_marker": "###################################################################################"
   },
   "source": [
    "This script tune DEEP_RF Moldel for Charged Particles and extract Weight Matrix #\n",
    "to Correct the loglikelihoods of Belle II Detectors                             #\n",
    "\t\t\t\t\t\t\t\t                  #\n",
    " Writtien by AlÃ¬ Bavarchee                                                      #\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95974467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import basf2 as b2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from argparse import ArgumentParser\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from .binary_tree import TorchDecisionTreeClassifier, TorchDecisionTreeRegressor\n",
    "#from .utils import sample_vectors, sample_dimensions\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from os import makedirs\n",
    "from os.path import join, dirname\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import linalg as la\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from os import makedirs\n",
    "from os.path import join, dirname\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df3eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _make_const_lists gives some cte values ; Particles and their corrispond PDG_code, Detectors\n",
    "def _make_const_lists():\n",
    "    \"\"\"Moving this code into a function to avoid a top-level ROOT import.\"\"\"\n",
    "    import ROOT.Belle2\n",
    "\n",
    "    PARTICLES, PDG_CODES = [], []\n",
    "    for i in range(len(ROOT.Belle2.Const.chargedStableSet)):\n",
    "        particle = ROOT.Belle2.Const.chargedStableSet.at(i)\n",
    "        name = (particle.__repr__()[7:-1]\n",
    "                .replace(\"-\", \"\")\n",
    "                .replace(\"+\", \"\")\n",
    "                .replace(\"euteron\", \"\"))\n",
    "        PARTICLES.append(name)\n",
    "        PDG_CODES.append(particle.getPDGCode())\n",
    "    # PARTICLES = [\"e\", \"mu\", \"pi\", \"K\", \"p\", \"d\"]\n",
    "    # PDG_CODES = [11, 13, 211, 321, 2212, 1000010020]\n",
    "\n",
    "    DETECTORS = []\n",
    "    for det in ROOT.Belle2.Const.PIDDetectors.set():\n",
    "        DETECTORS.append(ROOT.Belle2.Const.parseDetectors(det))\n",
    "    # DETECTORS = [\"SVD\", \"CDC\", \"TOP\", \"ARICH\", \"ECL\", \"KLM\"]\n",
    "\n",
    "    return PARTICLES, PDG_CODES, DETECTORS\n",
    "\n",
    "#This is a common pytorch data loader which loads data and splits them to train and test(val)\n",
    "def load_training_data(directory, p_lims=None, theta_lims=None, device=None):\n",
    "    \"\"\"Loads training and validation data within the given momentum and theta\n",
    "    limits (if given).\n",
    "\n",
    "    Args:\n",
    "        directory (str): Directory containing the train and validation sets.\n",
    "        p_lims (tuple(float), optional): Minimum and maximum momentum. Defaults\n",
    "            to None.\n",
    "        theta_lims (tuple(float), optional): Minimum and maximum theta in\n",
    "            degrees. Defaults to None.\n",
    "        device (torch.device, optional): Device to move the data onto. Defaults\n",
    "            to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Training log-likelihood data.\n",
    "        torch.Tensor: Training labels.\n",
    "        torch.Tensor: Validation log-likelihood data.\n",
    "        torch.Tensor: Validation labels.\n",
    "    \"\"\"\n",
    "    p_lo, p_hi = p_lims if p_lims is not None else (-np.inf, +np.inf)\n",
    "    t_lo, t_hi = theta_lims if theta_lims is not None else (-np.inf, +np.inf)\n",
    "    t_lo, t_hi = np.radians(t_lo), np.radians(t_hi)\n",
    "\n",
    "    def _load(filename):\n",
    "        data = np.load(filename)\n",
    "        X, y, p, t = data[\"X\"], data[\"y\"], data[\"p\"], data[\"theta\"]\n",
    "        mask = np.logical_and.reduce([p >= p_lo, p <= p_hi, t >= t_lo, t <= t_hi])\n",
    "        X = torch.tensor(X[mask]).to(device=device, dtype=torch.float)\n",
    "        y = torch.tensor(y[mask]).to(device=device, dtype=torch.long)\n",
    "        return X, y\n",
    "\n",
    "    X_tr, y_tr = _load(join(directory, \"train.npz\"))\n",
    "    X_va, y_va = _load(join(directory, \"val.npz\"))\n",
    "    return X_tr, y_tr, X_va, y_va\n",
    "\n",
    "data_folder = './data/slim_dstar'\n",
    "df = load_training_data(data_folder)\n",
    "\n",
    "X_tr, y_tr, X_va, y_va = load_training_data(data_folder)\n",
    "\n",
    "#mask pdg code of pion and kaon (target sets) to 1 and -1\n",
    "\n",
    "#Yy_tr = torch.where(y_tr==2,torch.tensor(1),y_tr)\n",
    "#Yy_tr = torch.where(y_tr==3,torch.tensor(-1),y_tr)\n",
    "\n",
    "#Yy_va = torch.where(y_va==2,torch.tensor(1),y_va)\n",
    "#Yy_va = torch.where(y_va==3,torch.tensor(-1),y_va)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# INPUTS:\n",
    "X_train= X_tr\n",
    "y_train=y_tr  #.view(-1)\n",
    "X_val= X_va\n",
    "y_val= y_va   #.view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4394d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MixedRegressor UPDATED*\n",
    "\n",
    "\n",
    "class MixedRegressor(nn.Module):\n",
    "    def __init__(self, n_class, n_detector, const_init=None, rf_params=None, pretrained_model=None):\n",
    "        super(MixedRegressor, self).__init__()  # Call the base class's constructor\n",
    "        \n",
    "        # Initialize attributes\n",
    "        self.n_class = n_class\n",
    "        self.n_detector = n_detector\n",
    "        self.const_init = const_init\n",
    "        self.rf_params = rf_params\n",
    "        \n",
    "        self.rf_net = None  # Initialize rf_net\n",
    "\n",
    "        if pretrained_model is not None:\n",
    "            self.neural_net = pretrained_model\n",
    "            self._initialize_pretrained_layers(n_class, n_detector)\n",
    "        else:\n",
    "            self.neural_net = models.resnet18(pretrained=False)\n",
    "            self._initialize_new_layers(n_class, n_detector)\n",
    "        self.fcs = nn.ModuleList([nn.Linear(n_detector, 1) for _ in range(n_class)])  # Initialize fcs\n",
    "\n",
    "        if self.const_init is not None:\n",
    "            self.const_layer = nn.Linear(self.n_detector, self.n_class)\n",
    "            self.const_layer.bias.data = torch.tensor([self.const_init] * self.n_class, dtype=torch.float32)\n",
    "        \n",
    "        if self.rf_params is not None:\n",
    "            self.random_forest = RandomForestRegressor(\n",
    "                n_estimators=self.rf_params['n_estimators'],\n",
    "                max_depth=self.rf_params['max_depth']\n",
    "            )\n",
    "            \n",
    "\n",
    "    def _initialize_new_layers(self, n_class, n_detector):\n",
    "        num_features = self.neural_net.fc.in_features\n",
    "        self.neural_net.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, n_class),  # Adjust output size for regression\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(n_class, n_detector * n_class)  # Output for each class-detector pair\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        n = self.n_detector\n",
    "        outs = [self.fcs[i](x[:, i * n: (i + 1) * n]) for i in range(self.n_class)]\n",
    "        out = torch.cat(outs, dim=1)\n",
    "        return out\n",
    "\n",
    "    def const_init(self, const):\n",
    "        with torch.no_grad():\n",
    "            for fc in self.fcs:\n",
    "                fc.weight.fill_(const)\n",
    "\n",
    "    def random_init(self, mean=1.0, std=0.5):\n",
    "        with torch.no_grad():\n",
    "            for fc in self.fcs:\n",
    "                fc.weight.fill_(0)\n",
    "                fc.weight.add_(torch.normal(mean=mean, std=std, size=fc.weight.size()))\n",
    "\n",
    "    def kill_unused(self, only):\n",
    "        if only is not None:\n",
    "            for i, pdg in enumerate(PDG_CODES):\n",
    "                if pdg in only:\n",
    "                    continue\n",
    "                self.fcs[i].weight.requires_grad = False\n",
    "                self.fcs[i].weight.fill_(1)\n",
    "\n",
    "    def train_neural(self, X_train, y_train, device='cpu', epochs=10, use_tqdm=True):\n",
    "        self.to(device=device)\n",
    "        opt = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=5e-4)\n",
    "        iterator = range(epochs)\n",
    "        neural_losses = []\n",
    "        rf_losses = []\n",
    "        if use_tqdm:\n",
    "            iterator = tqdm(iterator)\n",
    "\n",
    "        for epoch in iterator:\n",
    "            self.train()\n",
    "            opt.zero_grad()\n",
    "            out = self(X_train)\n",
    "            y_train_reshaped = y_train.float().reshape(-1, 1)\n",
    "            neural_loss_value = self.neural_loss(out, y_train_reshaped)\n",
    "            neural_losses.append(neural_loss_value.item())\n",
    "            # Perform the backward pass\n",
    "            neural_loss_value.backward()\n",
    "            #loss.backward()\n",
    "            opt.step()\n",
    "            # Calculate and record RF loss\n",
    "            # Record neural loss value\n",
    "            #rf_loss_value = self.rf_net.loss(X_train, y_train).item()\n",
    "            #rf_losses.append(rf_loss_value)\n",
    "            rf_losses.append(0)  # Placeholder since random forest doesn't use the same loss\n",
    "            \n",
    "\n",
    "    def neural_loss(self, output, target):\n",
    "        loss = nn.MSELoss()\n",
    "        return loss(output, target)\n",
    "\n",
    "    def train_rf(self, X_train, y_train):\n",
    "        if self.rf_net is None:\n",
    "            self.rf_net = RandomForestRegressor(**self.rf_params)  # Initialize rf_net if not already initialized\n",
    "        self.rf_net.fit(X_train, y_train)  # Train rf_net\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        neural_predictions = self.forward(X_test)\n",
    "        rf_predictions = self.rf_net.predict(X_test)\n",
    "        \n",
    "        # Detach tensors from the computation graph and then convert to NumPy arrays\n",
    "        neural_predictions_np = neural_predictions.detach().numpy()\n",
    "        rf_predictions_np = rf_predictions.reshape(-1, 1)  # Reshape to match neural_predictions\n",
    "        \n",
    "        combined_predictions = (neural_predictions_np + rf_predictions_np) / 2.0\n",
    "        return combined_predictions\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        predictions = self.predict(X_test)\n",
    "        #mse = mean_squared_error(y_test, predictions)\n",
    "        # Repeat each target value for each output unit\n",
    "        y_true_reshaped = np.repeat(y_test[:, np.newaxis], 6, axis=1)\n",
    "        mse_per_output = mean_squared_error(y_true_reshaped, predictions, multioutput='raw_values')\n",
    "        avg_mse = mse_per_output.mean()\n",
    "        return avg_mse\n",
    "\n",
    "    def get_weights(self, to_numpy=False, device=None):\n",
    "        neural_weights = None\n",
    "        if hasattr(self, 'neural_net'):\n",
    "            neural_weights = [param.detach().cpu().numpy() for param in self.neural_net.parameters()]\n",
    "   \n",
    "        rf_weights = self.rf_net.feature_importances_\n",
    "\n",
    "        if neural_weights is None:\n",
    "            combined_weights_matrix = rf_weights\n",
    "        else:\n",
    "            num_neural_weights = len(neural_weights)\n",
    "            num_rf_weights = len(rf_weights)\n",
    "            combined_weights_matrix = np.zeros((num_neural_weights, num_rf_weights))\n",
    "\n",
    "            for i in range(num_neural_weights):\n",
    "                for j in range(num_rf_weights):\n",
    "                    if neural_weights[i].shape == rf_weights[j].shape:\n",
    "                        combined_weights_matrix[i, j] = neural_weights[i] + rf_weights[j]\n",
    "                    else:\n",
    "                        # Handle the case where shapes are not compatible\n",
    "                        # You might need to reshape, slice, or modify the weights here\n",
    "                        combined_weights_matrix[i, j] = rf_weights[j]  # Placeholder value, adjust as needed\n",
    "\n",
    "        if to_numpy:\n",
    "            return combined_weights_matrix\n",
    "        else:\n",
    "            return torch.tensor(combined_weights_matrix, device=device)\n",
    "    \n",
    "    def save_model(self, file_path):\n",
    "        torch.save(self.neural_net.state_dict(), file_path)\n",
    "        \n",
    "    def load_model(self, file_path):\n",
    "        self.neural_net.load_state_dict(torch.load(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d96fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "#INI# TTTTex\n",
    "#####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "hyperparameters = {\n",
    "    'lr': [0.0005],\n",
    "    'hidden_units': [1],\n",
    "    'dropout': [0.0],\n",
    "    'rf_params': {\n",
    "        'n_estimators': [200],\n",
    "        'max_depth': [100],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize variables to store results\n",
    "mse_grid = np.zeros((len(hyperparameters['lr']), len(hyperparameters['hidden_units']),\n",
    "                     len(hyperparameters['dropout']), len(hyperparameters['rf_params']['n_estimators']),\n",
    "                     len(hyperparameters['rf_params']['max_depth'])))\n",
    "\n",
    "# Initialize variables to store ROC curves\n",
    "roc_curves = []\n",
    "# Initialize variables to store best model and its performance\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "best_mse = float('inf')\n",
    "best_auc = 0.0\n",
    "best_model = None\n",
    "best_weights = {}\n",
    "best_hyperparameters = {}\n",
    "\n",
    "# Initialize a subplot with multiple axes\n",
    "fig = make_subplots(rows=len(hyperparameters['hidden_units']), cols=len(hyperparameters['lr']),\n",
    "                    subplot_titles=[f\"Hidden Units: {hu}, Learning Rate: {lr}\" for hu in hyperparameters['hidden_units'] for lr in hyperparameters['lr']])\n",
    "\n",
    "# Loop through all combinations of hyperparameters\n",
    "for i, lr in enumerate(hyperparameters['lr']):\n",
    "    for j, hidden_units in enumerate(hyperparameters['hidden_units']):\n",
    "        # Initialize variables to store ROC curves\n",
    "        roc_curves = []\n",
    "        \n",
    "        for k, dropout in enumerate(hyperparameters['dropout']):\n",
    "            for m, n_estimators in enumerate(hyperparameters['rf_params']['n_estimators']):\n",
    "                for n, max_depth in enumerate(hyperparameters['rf_params']['max_depth']):\n",
    "                    # Instantiate the MixedRegressor with current hyperparameters\n",
    "                    model = MixedRegressor(\n",
    "                        n_class=6,\n",
    "                        n_detector=6,\n",
    "                        const_init=1,\n",
    "                        rf_params={\n",
    "                            'n_estimators': n_estimators,\n",
    "                            'max_depth': max_depth\n",
    "                        }, pretrained_model = None\n",
    "                    )\n",
    "\n",
    "                    # Train the model\n",
    "                    model.train_neural(X_train, y_train, device='cpu', epochs=100)\n",
    "                    model.train_rf(X_train, y_train)\n",
    "\n",
    "                    # Get the combined predictions\n",
    "                    combined_predictions = model.predict(X_val)\n",
    "                    \n",
    "                    # Calculate the model's performance on validation data\n",
    "                    mse = model.score(X_val, y_val)\n",
    "                    mse_grid[i, j, k, m, n] = mse\n",
    "                    \n",
    "\n",
    "                    # Update best model if the current model is better\n",
    "                    if mse < best_mse:\n",
    "                        best_mse = mse\n",
    "                        best_model = model\n",
    "                        best_hyperparameters = {\n",
    "                            'lr': lr,\n",
    "                            'hidden_units': hidden_units,\n",
    "                            'dropout': dropout,\n",
    "                            'n_estimators': n_estimators,\n",
    "                            'max_depth': max_depth\n",
    "                        }\n",
    "\n",
    "                    # Compute ROC curves and AUC for each class\n",
    "                    for class_idx in range(6):\n",
    "                        fpr, tpr, _ = roc_curve(y_val, combined_predictions[:, class_idx], pos_label=class_idx)\n",
    "                        roc_auc = auc(fpr, tpr)\n",
    "                        roc_curves.append((fpr, tpr, roc_auc))\n",
    "\n",
    "            # Plot ROC curves for each class\n",
    "            for class_idx in range(6):\n",
    "                fpr, tpr, roc_auc = roc_curves[class_idx]\n",
    "                trace = go.Scatter(x=fpr, y=tpr, mode='lines', name=f'Class {class_idx} (AUC = {roc_auc:.2f})')\n",
    "                fig.add_trace(trace, row=j+1, col=i+1)\n",
    "                \n",
    "                \n",
    "# Visualize the results using a heatmap\n",
    "best_indices = np.unravel_index(np.argmin(mse_grid), mse_grid.shape)\n",
    "best_mse = mse_grid[best_indices]\n",
    "best_hyperparameters = {\n",
    "    'lr': hyperparameters['lr'][best_indices[0]],\n",
    "    'hidden_units': hyperparameters['hidden_units'][best_indices[1]],\n",
    "    'dropout': hyperparameters['dropout'][best_indices[2]],\n",
    "    'n_estimators': hyperparameters['rf_params']['n_estimators'][best_indices[3]],\n",
    "    'max_depth': hyperparameters['rf_params']['max_depth'][best_indices[4]],\n",
    "}\n",
    "\n",
    "# Update subplot layout\n",
    "for row in range(1, len(hyperparameters['hidden_units'])+1):\n",
    "    for col in range(1, len(hyperparameters['lr'])+1):\n",
    "        fig.update_xaxes(title_text='False Positive Rate', row=row, col=col)\n",
    "        fig.update_yaxes(title_text='True Positive Rate', row=row, col=col)\n",
    "\n",
    "# Update main layout\n",
    "fig.update_layout(\n",
    "    title='ROC Curves for Different Hyperparameter Combinations',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Plot ROC curves for each class\n",
    "plt.figure(figsize=(10, 6))\n",
    "for class_idx in range(6):\n",
    "    fpr, tpr, roc_auc = roc_curves[class_idx]\n",
    "    plt.plot(fpr, tpr, label=f'Class {class_idx} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Each Class')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('FINX_ROC Curves_XIX06.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot Hyperparam GS Results\n",
    "fig.write_html('FINX_roc_curves_XIX06.html')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(np.log(mse_grid.mean(axis=4)[0, 0]), cmap='viridis')  # Taking log for better visualization\n",
    "plt.colorbar(label='Log Mean MSE')\n",
    "plt.xticks(range(len(hyperparameters['hidden_units'])), hyperparameters['hidden_units'])\n",
    "plt.yticks(range(len(hyperparameters['hidden_units'])), hyperparameters['hidden_units'])\n",
    "plt.xlabel('Dropout')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Hyperparam GS Results')\n",
    "plt.savefig('FINX_Hyperparam GS Results_XIX06.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(\"================================================\")\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "print(\"================================================\")\n",
    "print(\"Best Mean MSE:\", best_mse)\n",
    "print(\"================================================\")\n",
    "# Save the best model\n",
    "\n",
    "best_model.save_model('FINX_best_DeepRF_XIX6.pth')\n",
    "\n",
    "print(\"||||||||||||||||||||||\")\n",
    "\n",
    "# Extract the best model's weights\n",
    "best_model_keyz = best_model.neural_net.state_dict()\n",
    "kkeyz = [key for key in best_model_keyz.keys()]\n",
    "best_weights = best_model.neural_net.state_dict()[kkeyz[len(kkeyz)-1]]\n",
    "best_weights_reshaped = best_weights.reshape((6, 6))+1\n",
    "print(\"best_weights:\")\n",
    "print(best_weights_reshaped)\n",
    "np.save('FINX_best_weights_XIX06.npy', best_weights_reshaped)\n",
    "\n",
    "\n",
    "\n",
    "print('__MISSION COMPELETED__')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "sphinx"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
